{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08dafd87",
   "metadata": {},
   "source": [
    "## Language Model Representations of Ambiguous (Spanish) Nouns in Context\n",
    "\n",
    "Here, we load a monolingual Spanish-trained large language model (LLM) called [BETO](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased), and we use it to compute vector representations for target ambiguous Spanish nouns. To do so, we also load a dataframe of sentence pairs in Spanish, where each pair contains a target ambiguous noun whose sense is disambiguated by either an adjective or a verb (termed context cue). This context cue marks the only difference across a given pair of sentences. Context cues have been chosen such that sometimes the sentence pair evokes the same sense for the target word, **or** evokes different (homonymous or polysemous) senses for the target word. \n",
    "\n",
    "We run each (tokenized version of each) sentence through BETO, and extract the vector representation, or embedding, for the target noun from each of BETO's layers. We then compute and store the cosine distances between the target word embeddings from the first and second sentences of the pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66550e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? \n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # makes figs nicer!\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "sns.set(style='whitegrid',font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc552e",
   "metadata": {},
   "source": [
    "### define useful custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045d2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define useful custom functions to ...\n",
    "\n",
    "### ... find the target tokens within tokenized sequence\n",
    "def find_sublist_index(mylist, sublist):\n",
    "    \"\"\"Find the first occurence of sublist in list.\n",
    "    Return the start and end indices of sublist in list\"\"\"\n",
    "\n",
    "    for i in range(len(mylist)):\n",
    "        if mylist[i] == sublist[0] and mylist[i:i+len(sublist)] == sublist:\n",
    "            return i, i+len(sublist)\n",
    "    return None\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # This will cache results, handy later...\n",
    "\n",
    "\n",
    "### ... grab the embeddings for your target tokens\n",
    "def get_embedding(model, tokenizer, sentence, target, layer, device):\n",
    "    \"\"\"Get a token embedding for target in sentence\"\"\"\n",
    "    \n",
    "    # Tokenize sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Tokenize target\n",
    "    target_enc = tokenizer.encode(target, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=False).to(device)\n",
    "    \n",
    "    # Get indices of target in input tokens\n",
    "    target_inds = find_sublist_index(\n",
    "        inputs[\"input_ids\"][0].tolist(),\n",
    "        target_enc[0].tolist()\n",
    "    )\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        hidden_states = output.hidden_states\n",
    "\n",
    "    # Get layer\n",
    "    selected_layer = hidden_states[layer][0]\n",
    "\n",
    "    #grab just the embeddings for your target word's token(s)\n",
    "    token_embeddings = selected_layer[target_inds[0]:target_inds[1]]\n",
    "\n",
    "    #if a word is represented by >1 tokens, take mean\n",
    "    #across the multiple tokens' embeddings\n",
    "    embedding = torch.mean(token_embeddings, dim=0)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afeac0",
   "metadata": {},
   "source": [
    "### load your model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Models\n",
    "MODELS = [\"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "         \"bert-base-multilingual-cased\",\n",
    "         \"xlm-roberta-base\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e5233c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "### Define the url path to BETO\n",
    "\n",
    "mpath = \"dccuchile/bert-base-spanish-wwm-cased\" \n",
    "## Use \"bert-base-multilingual-cased\" for multlingual BERT comparison\n",
    "\n",
    "\n",
    "### Decide which device you want the models to run in\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "### Load your model & tokenizer\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(mpath,\n",
    "                                                          output_hidden_states=True)\n",
    "model.to(device) # allocate model to desired device\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(mpath)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a346b23",
   "metadata": {},
   "source": [
    "### load the dataframe of sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "680d3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/items/sawc_sentence_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947e7eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] # number of sentence pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58581c5",
   "metadata": {},
   "source": [
    "### compute cosine distances\n",
    "\n",
    "for each target word within a pair of sentences, for each model layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefad261",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the number of layers directly from the model specifications\n",
    "\n",
    "if mpath == \"xlm-roberta-base\":\n",
    "    n_layers = len(model.base_model.encoder.layer)\n",
    "else:\n",
    "    n_layers = len(model.bert.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c476ca16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce01efcebf4441299ce20af79aa498be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seantrott/anaconda3/lib/python3.11/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1578: UserWarning: cumsum_out_mps supported by MPS on MacOS 13+, please upgrade (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1699448804225/work/aten/src/ATen/native/mps/operations/UnaryOps.mm:406.)\n",
      "  incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b9662595204fe49e942540fca95b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45490930655646789509dbf33060fdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1523cac81e4d1bac006ea7386667aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e14c183d525410090e6e8728372184b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c943f9cee240e58448b74bce26f262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae42eb5eef0462883666000c6add907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d667ffa2be0c4dcba4eea2f875334aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4246c415a5144f019ede4ba674032d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4563e6a6fedb4008a5259da2eac7f3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46763c86a10d4563a16e7fec264f641a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db37d2b8d4f4ddc93eb576f47888918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2227bcb4c18749b195af3f948b699926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for layer in range(n_layers+1): # `range` is non-inclusive for the last value of interval\n",
    "    for (ix, row) in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "\n",
    "        ### Get embeddings for S1 and S2\n",
    "        s1 = get_embedding(model, tokenizer, row['Sentence_1'], row['Word'],layer, device)\n",
    "        s2 = get_embedding(model, tokenizer, row['Sentence_2'], row['Word'],layer, device)\n",
    "\n",
    "        ### Now calculate cosine distance \n",
    "        #.  note, tensors need to be copied to cpu to make this run\n",
    "        #.  still faster to do this copy than to just have everything\n",
    "        #.  running on the cpu\n",
    "        if device.type == \"mps\":  \n",
    "            model_cosine = cosine(s1.cpu(), s2.cpu())\n",
    "            \n",
    "        else: \n",
    "            model_cosine = cosine(s1, s2)\n",
    "\n",
    "\n",
    "        ### Figure out how many tokens you're\n",
    "        ### comparing across sentences\n",
    "        n_tokens_s1 = len(tokenizer.encode(row['Sentence_1']))\n",
    "        n_tokens_s2 = len(tokenizer.encode(row['Sentence_1']))\n",
    "\n",
    "        ### Add to results dictionary\n",
    "        results.append({\n",
    "            'Sentence_1': row['Sentence_1'],\n",
    "            'Sentence_2': row['Sentence_2'],\n",
    "            'Word': row['Word'],\n",
    "            'Same_sense': row['Same_sense'],\n",
    "            'Distance': model_cosine,\n",
    "            'Layer': layer,\n",
    "            'S1_ntokens': n_tokens_s1,\n",
    "            'S2_ntokens': n_tokens_s2\n",
    "        })\n",
    "        \n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['token_diffs'] = np.abs(df_results['S1_ntokens'].values-df_results['S2_ntokens'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8734883",
   "metadata": {},
   "source": [
    "### hurray! save your cosine distances to load in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4f4b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save your cosine distance results \n",
    "\n",
    "savepath = \"../data/processed/models/\"\n",
    "if not os.path.exists(savepath): \n",
    "    os.mkdir(savepath)\n",
    "    \n",
    "\n",
    "### Replace \"beto\" with name of model\n",
    "df_results.to_csv(os.path.join(savepath,\"xlm_sawc_distances.csv\"), index=False)\n",
    "# df_results.to_csv(os.path.join(savepath,\"beto_sawc_distances.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d4d78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
