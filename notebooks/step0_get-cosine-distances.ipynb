{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08dafd87",
   "metadata": {},
   "source": [
    "## Language Model Representations of Ambiguous (Spanish) Nouns in Context\n",
    "\n",
    "Here, we load a monolingual Spanish-trained large language model (LLM) called [BETO](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased), and we use it to compute vector representations for target ambiguous Spanish nouns. To do so, we also load a dataframe of sentence pairs in Spanish, where each pair contains a target ambiguous noun whose sense is disambiguated by either an adjective or a verb (termed context cue). This context cue marks the only difference across a given pair of sentences. Context cues have been chosen such that sometimes the sentence pair evokes the same sense for the target word, **or** evokes different (homonymous or polysemous) senses for the target word. \n",
    "\n",
    "We run each (tokenized version of each) sentence through BETO, and extract the vector representation, or embedding, for the target noun from each of BETO's layers. We then compute and store the cosine distances between the target word embeddings from the first and second sentences of the pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66550e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? \n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # makes figs nicer!\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "sns.set(style='whitegrid',font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc552e",
   "metadata": {},
   "source": [
    "### define useful custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045d2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define useful custom functions to ...\n",
    "\n",
    "### ... find the target tokens within tokenized sequence\n",
    "def find_sublist_index(mylist, sublist):\n",
    "    \"\"\"Find the first occurence of sublist in list.\n",
    "    Return the start and end indices of sublist in list\"\"\"\n",
    "\n",
    "    for i in range(len(mylist)):\n",
    "        if mylist[i] == sublist[0] and mylist[i:i+len(sublist)] == sublist:\n",
    "            return i, i+len(sublist)\n",
    "    return None\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # This will cache results, handy later...\n",
    "\n",
    "\n",
    "### ... grab the embeddings for your target tokens\n",
    "def get_embedding(model, tokenizer, sentence, target, layer, device):\n",
    "    \"\"\"Get a token embedding for target in sentence\"\"\"\n",
    "    \n",
    "    # Tokenize sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Tokenize target\n",
    "    target_enc = tokenizer.encode(target, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=False).to(device)\n",
    "    \n",
    "    # Get indices of target in input tokens\n",
    "    target_inds = find_sublist_index(\n",
    "        inputs[\"input_ids\"][0].tolist(),\n",
    "        target_enc[0].tolist()\n",
    "    )\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        hidden_states = output.hidden_states\n",
    "\n",
    "    # Get layer\n",
    "    selected_layer = hidden_states[layer][0]\n",
    "\n",
    "    #grab just the embeddings for your target word's token(s)\n",
    "    token_embeddings = selected_layer[target_inds[0]:target_inds[1]]\n",
    "\n",
    "    #if a word is represented by >1 tokens, take mean\n",
    "    #across the multiple tokens' embeddings\n",
    "    embedding = torch.mean(token_embeddings, dim=0)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afeac0",
   "metadata": {},
   "source": [
    "### load your model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3179d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Models\n",
    "MODELS = [\"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "         \"bert-base-multilingual-cased\",\n",
    "         \"xlm-roberta-base\",\n",
    "         \"dccuchile/albert-tiny-spanish\",\n",
    "          \"dccuchile/albert-base-spanish\",\n",
    "          \"dccuchile/albert-large-spanish\",\n",
    "          \"dccuchile/albert-xlarge-spanish\",\n",
    "          \"dccuchile/albert-xxlarge-spanish\",\n",
    "          \"PlanTL-GOB-ES/roberta-base-bne\",\n",
    "          \"PlanTL-GOB-ES/roberta-large-bne\",\n",
    "          \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
    "         \"dccuchile/distilbert-base-spanish-uncased \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5233c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '('dccuchile/bert-base-spanish-wwm-uncased',)'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    386\u001b[0m         path_or_repo_id,\n\u001b[1;32m    387\u001b[0m         filename,\n\u001b[1;32m    388\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    389\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    390\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    391\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    392\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    393\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    394\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    395\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    396\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    397\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:164\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: '('dccuchile/bert-base-spanish-wwm-uncased',)'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m### Load your model & tokenizer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# model = transformers.AutoModelForCausalLM.from_pretrained(mpath,output_hidden_states=True)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(mpath,output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# allocate model to desired device\u001b[39;00m\n\u001b[1;32m     17\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(mpath)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:488\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    489\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m    490\u001b[0m             CONFIG_NAME,\n\u001b[1;32m    491\u001b[0m             _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m             _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    494\u001b[0m         )\n\u001b[1;32m    495\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:450\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '('dccuchile/bert-base-spanish-wwm-uncased',)'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "### Define the url path to BETO\n",
    "\n",
    "mpath = \"dccuchile/bert-base-spanish-wwm-cased\" \n",
    "mpath = \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
    "## Use \"bert-base-multilingual-cased\" for multlingual BERT comparison\n",
    "\n",
    "\n",
    "### Decide which device you want the models to run in\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "### Load your model & tokenizer\n",
    "\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(mpath,output_hidden_states=True)\n",
    "model = transformers.AutoModel.from_pretrained(mpath,output_hidden_states=True)\n",
    "model.to(device) # allocate model to desired device\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(mpath)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b8f646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a346b23",
   "metadata": {},
   "source": [
    "### load the dataframe of sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "680d3436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/items/sawc_sentence_pairs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "947e7eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0] # number of sentence pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58581c5",
   "metadata": {},
   "source": [
    "### compute cosine distances\n",
    "\n",
    "for each target word within a pair of sentences, for each model layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e422873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Getting number of hidden layers \n",
    "### TODO: Double-check for all configurations\n",
    "n_layers = model.config.num_hidden_layers\n",
    "n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38daface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif mpath == \"xlm-roberta-base\":\\n    n_layers = len(model.base_model.encoder.layer)\\nelif mpath ==\"dccuchile/distilbert-base-spanish-uncased\":\\n    n_layers = len(model.base_model.transformer.layer)\\nelif mpath == \"dccuchile/albert-tiny-spanish\":\\n    n_layers = model.config.num_hidden_layers\\nelse:\\n    n_layers = len(model.bert.encoder.layer)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the number of layers directly from the model specifications\n",
    "\"\"\"\n",
    "if mpath == \"xlm-roberta-base\":\n",
    "    n_layers = len(model.base_model.encoder.layer)\n",
    "elif mpath ==\"dccuchile/distilbert-base-spanish-uncased\":\n",
    "    n_layers = len(model.base_model.transformer.layer)\n",
    "elif mpath == \"dccuchile/albert-tiny-spanish\":\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "else:\n",
    "    n_layers = len(model.bert.encoder.layer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a60387f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a13b6e2852405c95ac1f340f2c7214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0656b560589d4864bc9095be7a19146a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afc51ae5bd54bb1862288cccf117679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298edb5630dc44f68efb25b1eaabf4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8d6a0b0e534c5f8249768c7d3d3d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034ccb9f606a49da9b790d0a392a03bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45304c0810f9494ca6e32da0b527267d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4fa7faa0294bbb89adb4f368bdd08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff77988470944c798c5f7d07229b83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21efae74a2694590b20bf019b55137bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d21216b1734498bb44ae4d5ab4d4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3712691826a74eb6a30ed829026264cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6526afd1f25d4f25b22d021f735595cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830bd213026841e9b91da04ee21ee65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194c2e272ff74cf59db22fb6a8313ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea10777fdf2a468d8e7c90639b0a1f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7344dbf792414ee5a68313bf6b55a101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e024d9b765457ead30506e16ce78af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4232201150b140048b48b079c9420fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27e906150a148c587a167bbc1e07081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9253ed6e27ce4e1a9649273270544f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac18f4cfd6874a28859b7e9c2fe2f374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bc1f7357a44e00b8213c346e240e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f6194e8c98400398ccab53f55ece55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6ad720b2344bd1a0631cf6ab39cca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for layer in range(n_layers+1): # `range` is non-inclusive for the last value of interval\n",
    "    for (ix, row) in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "\n",
    "        ### Get embeddings for S1 and S2\n",
    "        \n",
    "        ## TODO: For RoBERTa\n",
    "        if mpath in [\"PlanTL-GOB-ES/roberta-base-bne\", \"PlanTL-GOB-ES/roberta-large-bne\"]:\n",
    "            target = \" {w}\".format(w = row['Word'])\n",
    "        else:\n",
    "            target = row['Word']\n",
    "        \n",
    "        s1 = get_embedding(model, tokenizer, row['Sentence_1'], target,layer, device)\n",
    "        s2 = get_embedding(model, tokenizer, row['Sentence_2'], target,layer, device)\n",
    "\n",
    "        ### Now calculate cosine distance \n",
    "        #.  note, tensors need to be copied to cpu to make this run\n",
    "        #.  still faster to do this copy than to just have everything\n",
    "        #.  running on the cpu\n",
    "        if device.type == \"mps\":  \n",
    "            model_cosine = cosine(s1.cpu(), s2.cpu())\n",
    "            \n",
    "        else: \n",
    "            model_cosine = cosine(s1, s2)\n",
    "\n",
    "\n",
    "        ### Figure out how many tokens you're\n",
    "        ### comparing across sentences\n",
    "        n_tokens_s1 = len(tokenizer.encode(row['Sentence_1']))\n",
    "        n_tokens_s2 = len(tokenizer.encode(row['Sentence_1']))\n",
    "\n",
    "        ### Add to results dictionary\n",
    "        results.append({\n",
    "            'Sentence_1': row['Sentence_1'],\n",
    "            'Sentence_2': row['Sentence_2'],\n",
    "            'Word': row['Word'],\n",
    "            'Same_sense': row['Same_sense'],\n",
    "            'Distance': model_cosine,\n",
    "            'Layer': layer,\n",
    "            'S1_ntokens': n_tokens_s1,\n",
    "            'S2_ntokens': n_tokens_s2\n",
    "        })\n",
    "        \n",
    "df_results = pd.DataFrame(results)\n",
    "df_results['token_diffs'] = np.abs(df_results['S1_ntokens'].values-df_results['S2_ntokens'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8734883",
   "metadata": {},
   "source": [
    "### hurray! save your cosine distances to load in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4f4b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save your cosine distance results \n",
    "\n",
    "savepath = \"../data/processed/models/\"\n",
    "if not os.path.exists(savepath): \n",
    "    os.mkdir(savepath)\n",
    "    \n",
    "\n",
    "### Replace \"beto\" with name of model\n",
    "df_results.to_csv(os.path.join(savepath,\"roberta-large_sawc_distances.csv\"), index=False)\n",
    "# df_results.to_csv(os.path.join(savepath,\"beto_sawc_distances.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fb69fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
