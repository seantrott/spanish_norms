{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08dafd87",
   "metadata": {},
   "source": [
    "## Language Model Representations of Ambiguous (Spanish) Nouns in Context\n",
    "\n",
    "Here, we load monolingual Spanish-trained, and multilingual large language models (LLMs) from the [BERT/BETO](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) family, and we use them to compute vector representations for target ambiguous Spanish nouns. To do so, we also load a dataframe of sentence pairs in Spanish, where each pair contains a target ambiguous noun whose sense is disambiguated by either an adjective or a verb (termed context cue). This context cue marks the only difference across a given pair of sentences. Context cues have been chosen such that sometimes the sentence pair evokes the same sense for the target word, **or** evokes different (homonymous or polysemous) senses for the target word. \n",
    "\n",
    "We run each (tokenized version of each) sentence through BETO and its variants, and extract the vector representation, or embedding, for the target noun from each of BETO's layers. We then compute and store the cosine distances between the target word embeddings from the first and second sentences of the pair. \n",
    "\n",
    "Here is a list of models we examined: \n",
    "\n",
    "* **BETO-cased** .... Cañete et al. (2020) *ICLR* [hugging-face](https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased) - [paper](https://arxiv.org/pdf/2308.02976.pdf)\n",
    "* **BETO-uncased** .... Cañete et al. (2020) *ICLR* [hugging-face](https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased) - [paper](https://arxiv.org/pdf/2308.02976.pdf)\n",
    "* **BERT-base-multilingual-cased\"** .... Devlin et al. (2019) *arXiv* [hugging-face](https://huggingface.co/google-bert/bert-base-multilingual-cased) -  [training-details](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages) - [paper](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "* **ROBERTa-BNE-base (MaRIa)** .... Gutiérrez-Fandiño et al. (2022) *Procesamiento del Lenguaje Natural* [hugging-face](https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne) - [paper](https://arxiv.org/pdf/2107.07253.pdf)\n",
    "* **ROBERTa-BNE-large (MaRIa)** .... Gutiérrez-Fandiño et al. (2022) *Procesamiento del Lenguaje Natural* - [hugging-face](https://huggingface.co/PlanTL-GOB-ES/roberta-large-bne) - [training-details](https://github.com/PlanTL-GOB-ES/lm-spanish?tab=readme-ov-file) - [paper](https://arxiv.org/pdf/2107.07253.pdf)\n",
    "* **XLM-ROBERTa-base** .... Conneau et al. (2020) *ACL* - [hugging-face](https://huggingface.co/FacebookAI/xlm-roberta-base) - [training-details](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr) - [paper](https://arxiv.org/pdf/1911.02116.pdf)\n",
    "* **ALBETO-tiny** .... Cañete et al. (2023) *arXiv* - [hugging-face](https://huggingface.co/dccuchile/albert-tiny-spanish) - [paper](https://arxiv.org/pdf/2204.09145.pdf)\n",
    "* **ALBETO-base** .... Cañete et al. (2023) *arXiv* - [hugging-face](https://huggingface.co/dccuchile/albert-base-spanish) - [paper](https://arxiv.org/pdf/2204.09145.pdf)\n",
    "* **ALBETO-large** .... Cañete et al. (2023) *arXiv* - [hugging-face](https://huggingface.co/dccuchile/albert-large-spanish) - [paper](https://arxiv.org/pdf/2204.09145.pdf)\n",
    "* **ALBETO-xlarge** .... Cañete et al. (2023) *arXiv* - [hugging-face](https://huggingface.co/dccuchile/albert-xlarge-spanish) - [paper](https://arxiv.org/pdf/2204.09145.pdf)\n",
    "* **ALBETO-xxlarge** .... Cañete et al. (2023) *arXiv* -  [hugging-face](https://huggingface.co/dccuchile/albert-xxlarge-spanish) - [paper](https://arxiv.org/pdf/2204.09145.pdf)\n",
    "* **DistilBETO-uncased** .... Cañete et al. (2023) *arXiv* -  [hugging-face](https://huggingface.co/dccuchile/distilbert-base-spanish-uncased/tree/main) - [paper](https://arxiv.org/pdf/2204.09145.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66550e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? \n",
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # makes figs nicer!\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "sns.set(style='whitegrid',font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc552e",
   "metadata": {},
   "source": [
    "### define useful custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "045d2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define useful custom functions to ...\n",
    "\n",
    "### ... find the target tokens within tokenized sequence\n",
    "def find_sublist_index(mylist, sublist):\n",
    "    \"\"\"Find the first occurence of sublist in list.\n",
    "    Return the start and end indices of sublist in list\"\"\"\n",
    "\n",
    "    for i in range(len(mylist)):\n",
    "        if mylist[i] == sublist[0] and mylist[i:i+len(sublist)] == sublist:\n",
    "            return i, i+len(sublist)\n",
    "    return None\n",
    "\n",
    "@functools.lru_cache(maxsize=None)  # This will cache results, handy later...\n",
    "\n",
    "\n",
    "### ... grab the embeddings for your target tokens\n",
    "def get_embedding(model, tokenizer, sentence, target, layer, device):\n",
    "    \"\"\"Get a token embedding for target in sentence\"\"\"\n",
    "    \n",
    "    # Tokenize sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Tokenize target\n",
    "    target_enc = tokenizer.encode(target, return_tensors=\"pt\",\n",
    "                                  add_special_tokens=False).to(device)\n",
    "    \n",
    "    # Get indices of target in input tokens\n",
    "    target_inds = find_sublist_index(\n",
    "        inputs[\"input_ids\"][0].tolist(),\n",
    "        target_enc[0].tolist()\n",
    "    )\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        hidden_states = output.hidden_states\n",
    "\n",
    "    # Get layer\n",
    "    selected_layer = hidden_states[layer][0]\n",
    "\n",
    "    #grab just the embeddings for your target word's token(s)\n",
    "    token_embeddings = selected_layer[target_inds[0]:target_inds[1]]\n",
    "\n",
    "    #if a word is represented by >1 tokens, take mean\n",
    "    #across the multiple tokens' embeddings\n",
    "    embedding = torch.mean(token_embeddings, dim=0)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "### ... grab the number of trainable parameters in the model\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"credit: https://stackoverflow.com/questions/49201236/check-the-total-number-of-parameters-in-a-pytorch-model\"\"\"\n",
    "    \n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        \n",
    "        # if the param is not trainable, skip it\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        \n",
    "        # otherwise, count it towards your number of params\n",
    "        params = parameter.numel()\n",
    "        total_params += params\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    \n",
    "    return total_params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059045c8",
   "metadata": {},
   "source": [
    "### load the dataframe of sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f466cb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stimpath = \"../data/raw/items/\"\n",
    "df = pd.read_csv(os.path.join(stimpath,\"sawc_sentence_pairs.csv\"))\n",
    "\n",
    "df.shape[0] # number of sentence pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4afeac0",
   "metadata": {},
   "source": [
    "### load your models and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3179d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the url paths to download your desired models\n",
    "#.  from Hugging Face\n",
    "\n",
    "MODELS = [\"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "          \"google-bert/bert-base-multilingual-cased\",\n",
    "          \"FacebookAI/xlm-roberta-base\",\n",
    "          \"dccuchile/albert-tiny-spanish\",\n",
    "          \"dccuchile/albert-base-spanish\",\n",
    "          \"dccuchile/albert-large-spanish\",\n",
    "          \"dccuchile/albert-xlarge-spanish\",\n",
    "          \"dccuchile/albert-xxlarge-spanish\",\n",
    "          \"PlanTL-GOB-ES/roberta-base-bne\",\n",
    "          \"PlanTL-GOB-ES/roberta-large-bne\",\n",
    "          \"dccuchile/bert-base-spanish-wwm-uncased\", \n",
    "          \"dccuchile/distilbert-base-spanish-uncased\",\n",
    "         \"dccuchile/patana-chilean-spanish-bert\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317198f1",
   "metadata": {},
   "source": [
    "### compute cosine distances\n",
    "\n",
    "for each target word within a pair of sentences, for each model layer, for each model specified in the `MODELS` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e5233c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7bb32f3e79460e8592fe49021b040e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5ade451fa9498396a645f02d9cbe82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bee2eb5bb224b7b953987e9ebb2dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/patana-chilean-spanish-bert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfd8632fed049bc944bd466880e3179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919686d8a75a47b2925b5ced15d3a470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b097b149baaa43218bd7728fd8a737de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/729k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efaa9228074540bfb333a100128ee97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of layers: 12\n",
      "Total Trainable Params: 109850880\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa377e809fc4106a46c504a2230d80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd3255da0e5467caf2164916fd4ef9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45933dcceaf4209a48c6cdd972b5fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4be2cb5c63047aab54c1b0b73613d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f4b5a0a42a444eba16ce712b848b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528fb3a041974b9c804e2cb24dae9c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d153c620d0640babd95e1322cc8b8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8b25359c1e48c78d20a469c2987384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e71a163be649b6868141a59249746b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc9239bac13494a8c647efd9302bc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e61af43b954537a3d3f58ae39e7c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439939b8537144af838cc0a0bd2c45b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e11d564a7294a9fa22d6c98f4e08d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Iterate over models and do the work! \n",
    "\n",
    "for mpath in tqdm(MODELS,colour=\"cornflowerblue\"):\n",
    "\n",
    "    ### Decide which device you want the models to run in\n",
    "    \n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    ### Load your model & tokenizer\n",
    "    \n",
    "    model = transformers.AutoModel.from_pretrained(mpath,output_hidden_states=True)\n",
    "    model.to(device) # allocate model to desired device\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(mpath)  \n",
    "    \n",
    "    \n",
    "    ### Get the number of layers & params directly from the model specifications\n",
    "    \n",
    "    # TODO: Double-check for all configurations\n",
    "    \n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    print(\"number of layers:\", n_layers)\n",
    "\n",
    "    n_params = count_parameters(model)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for layer in range(n_layers+1): # `range` is non-inclusive for the last value of interval\n",
    "        for (ix, row) in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "\n",
    "            ### Get embeddings for S1 and S2\n",
    "\n",
    "            # note: account for tokenization differences in RoBERTa Spanish monolinguals  by\n",
    "            #.      adding a whitespace in front of the target word (otherwise, the function\n",
    "            #.      `find_sublist_index` will not be able to identify the target token-s within\n",
    "            #.      the tokenized sentence)\n",
    "            \n",
    "            if mpath in [\"PlanTL-GOB-ES/roberta-base-bne\", \"PlanTL-GOB-ES/roberta-large-bne\"]:\n",
    "                target = \" {w}\".format(w = row['Word'])\n",
    "            else:\n",
    "                target = row['Word']\n",
    "\n",
    "            s1 = get_embedding(model, tokenizer, row['Sentence_1'], target,layer, device)\n",
    "            s2 = get_embedding(model, tokenizer, row['Sentence_2'], target,layer, device)\n",
    "\n",
    "            ### Now calculate cosine distance \n",
    "            #.  note, tensors need to be copied to cpu to make this run;\n",
    "            #.  still faster to do this copy than to just have everything\n",
    "            #.  running on the cpu\n",
    "            if device.type == \"mps\":  \n",
    "                model_cosine = cosine(s1.cpu(), s2.cpu())\n",
    "\n",
    "            else: \n",
    "                model_cosine = cosine(s1, s2)\n",
    "\n",
    "\n",
    "            ### Figure out how many tokens you're\n",
    "            ### comparing across sentences\n",
    "            n_tokens_s1 = len(tokenizer.encode(row['Sentence_1']))\n",
    "            n_tokens_s2 = len(tokenizer.encode(row['Sentence_2']))\n",
    "\n",
    "            ### Add to results dictionary\n",
    "            results.append({\n",
    "                'Sentence_1': row['Sentence_1'],\n",
    "                'Sentence_2': row['Sentence_2'],\n",
    "                'Word': row['Word'],\n",
    "                'Same_sense': row['Same_sense'],\n",
    "                'Distance': model_cosine,\n",
    "                'Layer': layer,\n",
    "                'S1_ntokens': n_tokens_s1,\n",
    "                'S2_ntokens': n_tokens_s2\n",
    "            })\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results['token_diffs'] = np.abs(df_results['S1_ntokens'].values-df_results['S2_ntokens'].values)\n",
    "    df_results['n_params'] = np.repeat(n_params,df_results.shape[0])\n",
    "    \n",
    "    \n",
    "    ### Hurray! Save your cosine distance results to load into R\n",
    "    #.  for analysis\n",
    "\n",
    "    savepath = \"../data/processed/models/\"\n",
    "    if not os.path.exists(savepath): \n",
    "        os.mkdir(savepath)\n",
    "\n",
    "    filename = \"sawc-distances_model-\" + mpath.split(\"/\")[1] + \".csv\"\n",
    "\n",
    "    df_results.to_csv(os.path.join(savepath,filename), index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38daface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif mpath == \"xlm-roberta-base\":\\n    n_layers = len(model.base_model.encoder.layer)\\nelif mpath ==\"dccuchile/distilbert-base-spanish-uncased\":\\n    n_layers = len(model.base_model.transformer.layer)\\nelif mpath == \"dccuchile/albert-tiny-spanish\":\\n    n_layers = model.config.num_hidden_layers\\nelse:\\n    n_layers = len(model.bert.encoder.layer)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# if mpath == \"xlm-roberta-base\":\n",
    "#     n_layers = len(model.base_model.encoder.layer)\n",
    "# elif mpath ==\"dccuchile/distilbert-base-spanish-uncased\":\n",
    "#     n_layers = len(model.base_model.transformer.layer)\n",
    "# elif mpath == \"dccuchile/albert-tiny-spanish\":\n",
    "#     n_layers = model.config.num_hidden_layers\n",
    "# else:\n",
    "#     n_layers = len(model.bert.encoder.layer)\n",
    "# \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
